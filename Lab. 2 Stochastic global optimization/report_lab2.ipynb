{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Lab. 2: Stochastic global optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "- CHECK ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eps` parameter: following the documentation this parameter determine how \"strong\" the convergence criterion is.\n",
    "With smaller value of `eps` in fact we can notice that the performance **tends** to improve due to the high degree of convergence set. So, on the other side, putting `eps` too high will result in early terminate of the algorithm, maybe with a not fully optimum converged value. \n",
    "\n",
    "The lab studies show that this **parameter'influence is really related to the benchmark function selected but in some case the improvements of the results are evident.** \n",
    "\n",
    "For example using `EggHolder()` as benchmark function with 300 iterations and moving `eps` from 1 to 0.1:\n",
    "\n",
    "```python\n",
    "Using eps = 1 # same with 100\n",
    "Best value: -724.8978437065213\n",
    "Best location: [-341.33333333 -455.11111111]\n",
    "--------------------------------------------\n",
    "Using eps = 0.1 # same with 0.00001\n",
    "Best value: -931.5964679665101\n",
    "Best location: [467.75308642 455.11111111]\n",
    "```\n",
    "\n",
    "As sad before the choice of the benchmark function is relevant, for example with *similar* function like the `Ackley` and the `GoldsteinAndPrice`, that are both non-convex and multimodal with global minimum near local optima, increasing eps there are not changes, with the `Himmelblau` function the improvements are really small:\n",
    "\n",
    "```python\n",
    "Ackley: eps = [0.1, 1] => [0.0309, 0.0309]\n",
    "# no improvement, same with GoldsteinAndPrice\n",
    "--------------------------------------------\n",
    "Himmelblau: eps = [0.1, 1] => [0.0785, 0.0012] \n",
    "# small improvement\n",
    "```\n",
    "\n",
    "\n",
    "It has been noticed that setting increasingly smaller values (e.g. from 0.1 to 0.00000000000001) there are not changes in general or the changes are really small, same thing with \"from-large-to-very-large\" values. Probably there is a sort of normalization / bound range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An high value of the `max_iter` parameter, as seen in other methods, lead to better result since allow to explore different regions of the search space and so can potentially converge closer to the optimal solution. On the other side, too many iterations can be useless since the optimum can be already discovered and, finally, this parameter has an impact on the computational cost, obliviously.\n",
    "\n",
    "Here we can say that using a `Ackley` function increasing the number of iterations is useless, instead with the `EggHolder` result is better increasing from 10 to 100, but no from 100 to 1000.\n",
    "\n",
    "```python\n",
    "Ackley: max_iter = [10, 100, 1000] => [0.0309, 0.0309, 0.0309]\n",
    "# no improvement, same with GoldsteinAndPrice\n",
    "--------------------------------------------\n",
    "EggHolder: max_iter = [10, 100, 1000]: [-573.125, -931.596, -931.596] \n",
    "# 10-100: improvement, 100-1000: no improvement\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIN-HOPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature `T` controls the exploration-exploitation trade-off: higher T encourages exploration, while lower values favor exploitation.\n",
    "\n",
    "Here all is about the function points locations, in same case we need to escape from local minima ( = large value of T), in other configurations instead we want to search around the local minima to refine the solution ( = smaller value of T).\n",
    "Find the balance is the key.\n",
    "\n",
    "Report shows that **functions with some peaks and valley, like the `Rastrigin` or the `Ackley` are more effected by this parameter then the more \"smooth\" ones, like the `Rosenbrock`.**\n",
    "\n",
    "```python\n",
    "Rastrigin: T = [0.1, 1.0, 10.0] => [2.131e-14, 4.974, 1.999]\n",
    "# from e-14 to 4.9\n",
    "--------------------------------------------\n",
    "Rosenbrock: T = [0.1, 1.0, 10.0] => [1.981e-12, 2.011e-12, 4,112e-12]\n",
    "# all near e-12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also for the `stepsize` parameter, larger step sizes allow for more extensive exploration, so again non-smooth functions with multiple local minima are more influenced by changing this configuration.\n",
    "\n",
    "```python\n",
    "Rastrigin: T = [10, 100] => [2.131e-14, 0.995]\n",
    "# from e-14 to 0.99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of iterations influences the trade-off between exploration and exploitation. Similar to the temperature higher value allows the algorithm to explore more and find the balance, like for the temperature again, is fundamental. \n",
    "\n",
    "Report shows that **no smooth functions are more sensitive**, exactly like with the temperature but the improvements seems to be more significative, but it's important to consider the **increasing cost** due to more iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point `x_0` significantly affects the algorithm. If the initial point is close to a local minimum, the algorithm may converge prematurely but too far really affect the computational cost of the exploration. Setting the `x_0` in an appropriate way thank to the prior visual knowledge is important. That holds for all the benchmark functions and **it seems the most crucial parameter**.\n",
    "\n",
    "Here a simple example with `EggHolder`\n",
    "\n",
    "```python\n",
    "x_0 = [-300, 200] => -558.5207320973292 # local\n",
    "x_0 = [-420, 400] => -894.5789003905979 # near the global\n",
    "```\n",
    "\n",
    "Futhermore I've tried to lunch different benchmark functions with randomly starting points and the best values founded changes a lot due to the initialization of `x_0`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Powell and Nelder-Mead methods, also for the DIRECT and Basin-Hopping the **fundamental thing is the setting of the parameters and the objective function played**, with its peculiarities. It's quite difficult to compare directly these different algorithms, even trying to adapt the parameters to be more \"similar\" as possible in order to make performance evaluations. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
